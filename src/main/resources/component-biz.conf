product {
    modules {
        sirius-biz {
            version = "${project.version}"
            build = "${build.number}"
            date = "${timestamp}"
            vcs = "${build.vcs.number}"
        }
    }
}

# sirius-biz provides a multitude of frameworks for different use-cases
# and implements them using different databases. Therefore by default
# all frameworks are disabled and have to be enabled by the application.
sirius.frameworks {

    # User-manager which supports multi-tenant applications
    biz.tenants = false

    # Enables the JDBC storage layer for tenants.
    biz.tenants-jdbc = false

    # Enables the MongoDB storage layer for tenants.
    biz.tenants-mongo = false

    # A framework for letting a user map codes to text values
    biz.code-lists = false

    # Enables the JDBC storage layer for code-lists.
    biz.code-lists-jdbc = false

    # Enables the MongoDB storage layer for code-lists.
    biz.code-lists-mongo = false

    # Utilizes Elasticsearch to store all recorded logs, incidents,
    # audit logs and mails
    biz.protocols = false

    # Provides a change log for all entities which include a JournalData
    biz.journal = false

    # Provides an ID Generator which can either use MongoDB or JDBC
    # to generate sequences of unique IDs
    biz.sequences = false

    # Provides distributed locks based on either the JVM, Redis or JDBC
    biz.locks = false

    # Provides a object store like API for storing files. This can either use
    # the file system or other storage facilities.
    biz.storage = false

    # Provides a rate-limiting / firewall which is either based on Redis.
    biz.isenguard = false

    # Provides a framework to record and visualize the output of background processes.
    biz.processes = false

    # Provides a framework to execute all kinds of application defined jobs.
    biz.jobs = false

    # Provides a storage option to place execution flags in a JDBC database.
    biz.analytics.execution-flags-jdbc = false

    # Provides a storage option to place execution flags in a MongoDB.
    biz.analytics.execution-flags-mongo = false

    # Provides a storage option for metrics in a JDBC database.
    biz.analytics.metrics-jdbc = false

    # Provides a storage option for in a MongoDB.
    biz.analytics.metrics-mongo = false
}

# Place the local address of the node here, i.e. http://192.168.0.1
# If no address is given, we use the local address determined by the system
# (using: InetAddress.getLocalHost().getHostAddress()) - however, in some
# environments like Docker, this might yield an inappropriate address.
#
# This address has to be reachable from all other cluster nodes.
sirius.nodeAddress = ""

# Contains settings for the built-in firewall and rate-limiting facility
isenguard {
    # Determines which limiter is used. By default we use a "smart" strategy,
    # which uses "redis" is available and otherwise switches to the "noop" limiter.
    limiter = "smart"

    # If the "Redis" limiter is used, the given redis database is used to store
    # the counters and blocked IPs. By default we use the "system" database,
    # which is the default redis.
    redisName = "system"

    # Contains an interval and limit per interval for each realm.
    # Note that the realm "http" is used to limit all notable
    # HTTP calls.
    limit {
        # Specifies the defaults for all realms unless noted otherwise.
        # By default IsenGuard is turned off.
        default {
            interval = 10m
            limit = 0
        }

        # Specifies the constraints for all HTTP requests. By default this
        # is turned off, as there is no way of knowing the usage pattern
        # of a specific application.
        http {
            interval = 0
            limit = 0
        }

        # Specifies the constraints for negative AuditLog events (wrong password etc).
        # Once this limit is hit, the calling IP will be blocked for ten minutes.
        security {
            interval = 2m
            limit = 50
        }
    }

}

health.limits {
    # If there is any lock held, we will report this - but there is no
    # sane limit how many locks can be considered healthy / unhealty
    locks-count.gray  = 1
    locks-count.warning  = 0
    locks-count.error = 0

    # We start to warn as soon as we encounter one long running lock
    # (held for at least 30min). As this can still be quite alright
    # we do not consider this critical (red)
    locks-long-running.gray  = 0
    locks-long-running.warning  = 1
    locks-long-running.error = 0

    # Monitors the utilization of the events buffer which is used by the
    # EventRecorder to permit batch inserts of recorded events into Clickhouse
    events-buffer-usage.gray = 0
    events-buffer-usage.warning = 80
    events-buffer-usage.error = 99
}

async {
    # Defines the maximal number of concurrent tasks executed for the
    # Distributed Tasks framework.
    executor.distributed-tasks {
        poolSize = 8

        # Having a queue would be pointless, as the WorkLoaderLoop only
        # tries to keep the available executors running but will not
        # schedule additional work.
        queueLength = 0
    }

    # Interctive jobs should actually execute quite instantly. Therefore
    # we only permit a low parallelism but a certain queue length for peak loads.
    executor.interactive-jobs {
        poolSize = 2
        queueLength = 100
    }

    distributed {
        # Configures the nature of the queues used to distribute tasks.
        queues {
            # Each queue needs to suppliy the following settings
            # example {
                # Contains the concurrency token to control node-local parallelism
                # concurrencyToken = SomeToken

                # Determines if the queue is prioritized or a FIFO queue
                # prioritized = false

                # For prioritized queues the penalty should approximately be
                # equal to the expected runtime of an average task. This time is
                # used to compute the effective execution priority once a task is
                # scheduled.
                # penaltyTime = 1 minute
            # }

            # Defines the queue used by the MetricsGuaranteedSchedulerExecutor when scheduling metrics.
            metrics-scheduler {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the MetricsGuaranteedBatchExecutor when executing batches of metrics tasks.
            metrics-batch {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the MetricsBestEffortSchedulerExecutor and MetricsBestEffortBatchExecutor
            # when scheduling and executing batches of "best effort" metrics tasks.
            #
            # These tasks are only scheduled if the queue is empty. Therefore, if the system is overloaded it may
            # skip some of the "best effort" tasks - but it will never skip a guaranteed task.
            metrics-best-effort {
                concurrencyToken = "analytics"
                prioritized = false
            }

            # Defines the queue used by the DefaultBatchProcessTaskExecutor for miscellaneous jobs.
            jobs {
                # We use a simple token here so that the number of parallel jobs can be specified
                concurrencyToken = "small-jobs"

                # The queue will be prioritized
                prioritized = true

                # As we have no idea what the average runtime of a job in this queue might be, we default the penalty
                # time to one hour.
                penaltyTime = 1h
            }

            # Defines the queue used by the ImportBatchProcessTaskExecutor for import jobs.
            import-jobs {
                concurrencyToken = "large-jobs"
                prioritized = true
                penaltyTime = 2h
            }

            # Defines the queue used by the ExportBatchProcessTaskExecutor for export jobs.
            export-jobs {
                concurrencyToken = "small-jobs"
                prioritized = true
                penaltyTime = 30m
            }

            # Defines the queue used by the ReportBatchProcessTaskExecutor for report jobs.
            report-jobs {
                concurrencyToken = "small-jobs"
                prioritized = true
                penaltyTime = 30m
            }

            # Defines the queue used by the CheckBatchProcessTaskExecutor for check jobs.
            check-jobs {
                concurrencyToken = "small-jobs"
                prioritized = true
                penaltyTime = 1h
            }
        }

        # Configures concurrency tokens which are semaphores on each node and
        # control local parallelism. Note that a single token can be shared by
        # multiple queues.
        concurrency {
            # Specifies the maximal number of parallel small jobs.
            small-jobs = 4

            # Specifies the maximal number of parallel large jobs.
            large-jobs = 2

            # Specifies the maximal number of parallel analytical tasks.
            analytics = 2
        }
    }
}

# Provides a cluster wide controller for executing background jobs.
# NeighborhoodWatch uses Redis locks and timestamps to control the
# execution of background jobs across a cluster of nodes.
# Per job one for the following settings can be set:
# LOCAL    - the jobs runs on this node independently of the cluster
# CLUSTER  - the job may run on this node, but only on one node within the cluster at once
# DISABLED - the job is disabled on this node
orchestration {
    loop-elastic-auto-batch = LOCAL
    loop-event-processor = LOCAL
    loop-delay-line = LOCAL
    loop-distributed-tasks-work-loader = LOCAL
    loop-redis-limiter-cleanup = CLUSTER
    task-protocols-cleaner = CLUSTER
    task-execution-flags-cleaner = CLUSTER
    task-logs-cleanup = LOCAL
    task-storage-cleaner = CLUSTER
}

timer.daily {

    # Determines when protocols and journals are purged based on the given settings...
    protocols-cleaner = 2

    # Determines when old execution flags are purged...
    delete-execution-flags = 4

    # Determines when outdated files in the storage system are purged...
    storage-cleaner = 3

    # Determines when analytical tasks are scheduled...
    analytical-engine = 23

}

# Controls the storage duration of protocol entries
protocols {
    keep-logs = 30 days
    keep-incidents = 30 days
    keep-mails = 365 days
    keep-journal = 1000 days
    keep-neutral-audit-logs = 30 days
    keep-negative-audit-logs = 180 days
}

# Specifies the secret used to sign journal URLs for specific entities...
# An empty secret signals, that a new (local) secret es generated during startup...
journal.secret = ""

jobs.categories {
    import {
        label = "$JobCategory.import"
        priority = 100
        icon = "fa-upload"
    }
    export {
        label = "$JobCategory.export"
        priority = 200
        icon = "fa-download"
    }
    check {
        label = "$JobCategory.check"
        priority = 300
        icon = "fa-check-square-o"
    }
    report {
        label = "$JobCategory.report"
        priority = 400
        icon = "fa-line-chart"
    }
    misc {
        label = "$JobCategory.misc"
        priority = 500
        icon = "fa-cogs"
    }
}

content.extensions {
    templates-page-menu-left {
        settings {
            priority = 1000
            template = "templates/wondergem/menu-left-settings.html.pasta"
        }
    }

    templates-menu-settings {
        code-lists {
            priority = 100
            template = "templates/biz/codelists/settings-menu-code-lists.html.pasta"
        }
    }

    templates-page-menu-right {
        user {
            priority = 500
            template = "templates/wondergem/menu-right-user.html.pasta"
        }
    }

    templates-user-menu {
        tenants {
            priority = 100
            template = "templates/biz/tenants/user-menu-tenants.html.pasta"
        }
    }

    templates-page-footer {
        stored-object-uploader {
            priority = 100
            template = "templates/biz/storage/stored-object-uploader.html.pasta"
        }

        virtual-object-autocomplete-script {
            priority = 100
            template = "templates/biz/jobs/params/virtual-object-autocomplete-script.html.pasta"
        }
    }

}

code-lists {
    default {
        autofill = true
    }

    salutations {
        name = "Salutations"
        description = "Contains all salutations known to the system"
    }

    countries {
        name = "Countries"
        description = "Contains all countries known to the system. A RegEx can be supplied as additional value which is used to verify ZIP codes"
    }
}

storage {

    # Defines the base directory when storing buckets in disk.
    baseDir = "data/storage"

    # If using ImageMagick, consider a command like:
    # "convert ${src} -resize ${width}x${height}> -quiet -quality 89 -format ${imageFormat} -strip -colorspace RGB -background white ${extend} -flatten ${dest}"
    conversionCommand=""

    # Option for the conversion command to extend the image to a minimum size
    extendOption = "-gravity center -extent ${extendWidth}x${extendHeight}<"

    # Defines all buckets known to the system.
    buckets {
        default {
            # Defines the permission required to view the bucket in the management UI.
            permission = "permission-manage-files"

            # Determines if an object (file) can be created via the management UI.
            canCreate = false

            # Determines if an object (file) can be edited via the management UI.
            canEdit = false

            # Determines whether a search in a bucket should always use a like constraint.
            alwaysUseLikeSearch = false

            # Determines if an object (file) can be deleted via the management UI.
            canDelete = false

            # Determines if objects are automatically removed after N days. 0 means disabled.
            deleteFilesAfterDays = 0

            # Determines the storage engine used for the bucket.
            engine = "fs"
        }

        # A work directory / bucket is provided per tenant and can be used to in- and output files.
        # This is also visible in the built-in virtual file system (FTP server) to upload and download files.
        # To limit the number of files in this directory, old files (older than 30 days) are automatically removed.
        # Therefore this should not be used for permanent storage.
        work {
            canCreate = true
            canEdit = true
            canDelete = true
            deleteFilesAfterDays = 30
        }

        # Provides a temporary storage space which is automatically maintained (files are deleted after 30 days).
        tmp {
            permission = "permission-manage-admin-files"
            canCreate = true
            canEdit = true
            canDelete = true
            deleteFilesAfterDays = 30
        }

        # Defines storage for versioned files
        versioned-files {
            canCreate = false
            canEdit = false
            canDelete = false

            # number of versions kept from one versioned file
            # setting this number to 0 will keep all versions
            maxNumberOfVersions = 50
        }
    }

}

# Contains settings for the virtual file system.
vfs {

    # Defines the settings of the built-in FTP server.
    ftp {
        # Specifies the port to listen on. Use 0 to disable the server or 21 to run it on the common FTP port.
        port = 0

        # Specifies the port range to listen on for the data connection in passive mode, use any by default
        # Examples:
        # 2300               only use port 2300 as the passive port
        # 2300-2399          use all ports in the range
        # 2300-              use all ports larger than 2300
        # 2300, 2305, 2400-  use 2300 or 2305 or any port larger than 2400
        passivePorts = ""

        # Specifies the IP address to bind to. Leave empty to use all IP addresses.
        bindAddress = ""

        # Specifies the IP address clients have to connect to in passive mode.
        passiveExternalAddress = ""

        # Specifies the max. login failures before disconnecting.
        maxLoginFailures = 5

        # Specifies the max. number of concurrent clients.
        maxClients = 100

        # Specifies the max. number of threads to utilize.
        maxThreads = 10

        # Specifies the idle timeout for connections.
        idleTimeout = 10m

        # Specifies the max. connections per IP.
        maxConnectionsPerIp = 5

        # Specifies the JKS keystore to use for FTPS.
        keystore = ""

        # Specifies the keystore password.
        keystorePassword = ""

        # Specifies the key alias to use.
        keyAlias = ""

        # Determines if FTPS should be forced or not.
        forceSSL = false
    }
}

# Provides credentials for the S3 compatible stores managed by ObjectStores.
s3 {

    stores {

        # Provides the default configuration shared by all stores.
        default {
            accessKey = ""
            secretKey = ""
            endPoint = ""
            bucketSuffix = ""
            pathStyleAccess = true

            # Specifies the signer to use. Leave empty to use the standard signer of the
            # current AWS SDK.
            signer = ""
            # Use the following setting for CEPH stores:
            # signer = "S3SignerType"
        }

        # By default a "system" store is used if no other name is given.
        # An application should provide a configuration for this store if ObjectStores are used.
        system {

        }
    }

}

security {

    passwordMinLength = 4
    passwordSaneLength = 6

    # Specifies for how long generated passwords should be displayed.
    showGeneratedPasswordFor = 5 days

    scopes.default {
        manager = "tenants"
        system-tenant = "1"
        loginCookieTTL = 90 days
    }

    permissions {
        permission-manage-system        : "Required for system tenant user accounts to manage system settings"
        permission-manage-tenants       : "Required to manage tenants of the system"
        permission-manage-user-accounts : "Required to manage user accounts"
        permission-manage-code-lists    : "Required to manage code lists"
        permission-system-protocols     : "Required to view protocols like logs, errors, mails, all audit logs"
        permission-system-cluster       : "Required to view and manage the cluster state"
        permission-audit-logs           : "Required to view audit logs for the own tenant"
        permission-system-journal       : "Required to view the system journal"
        permission-select-tenant        : "Required to switch to another tenant"
        permission-select-user-account  : "Required to switch to another user"
        permission-tasks                : "Required to view all managed tasks"
        permission-manage-jobs          : "Required to edit and create jobs"
        permission-execute-jobs         : "Required to execute jobs"
        permission-manage-files         : "Required to manage well known buckets in the storage system"
        permission-manage-admin-files   : "Required to access administrative buckets in the storage system"
        permission-manage-processes     : "Required to view processes of other users within the same tenant"
        permission-manage-all-processes : "Required to view processes of all users and tenants"
    }

    roles = [ "user-administrator", "administrator", "jobs-manager", "jobs-execution", "file-manager" ]

    tenantPermissions = [
        "feature-provide-jobs"
    ]

    profiles {

        flag-system-tenant {
            permission-manage-tenants = true
            permission-manage-code-lists = true
            permission-system-protocols = true
            permission-system-cluster = true
            permission-system-journal = true
            permission-system-console = true
            permission-system-timing = true
            permission-system-scripting = true
            permission-system-notify-state = true
            permission-system-load = true
            permission-tasks = true
            permission-manage-all-processes = true
            feature-provide-jobs = true
        }

        user-administrator {
            permission-manage-user-accounts = true
            permission-select-user-account = true
            permission-manage-system = true
        }

        administrator {
            permission-select-tenant = true
            permission-execute-jobs = true
            permission-manage-jobs = true
            permission-view-scope-default-config = true
            permission-manage-jobs = true
            permission-manage-files = true
            permission-manage-admin-files = true
            permission-system-audit-logs = true
            permission-manage-processes = true
        }

        jobs-manager {
            permission-manage-jobs = true
        }

        jobs-execution {
            permission-execute-jobs = true
        }

        file-manager {
            permission-manage-files = true
        }
    }

}

# Specifies cache sizes used by the biz platform
cache {

    tenants-users {
        maxSize = 100
        ttl = 1 hour
    }

    tenants-roles {
        maxSize = 100
        ttl = 1 hour
    }

    tenants-children {
        maxSize = 256
        ttl = 1 hour
    }

    tenants-tenants {
        maxSize = 32
        ttl = 1 hour
    }

    tenants-configs {
        maxSize = 100
        ttl = 1 hour
    }

    storage-object-metadata {
        maxSize = 16384
        ttl = 1 hour
    }

    virtual-objects {
        maxSize = 16384
        ttl = 1 hour
    }

    processes-first-level {
        maxSize = 128
        ttl = 10 seconds
    }

    processes-second-level {
        maxSize = 256
        ttl = 10 minutes
    }

    codelists-values {
        maxSize = 4096
        ttl = 1 hour
    }

    objectstores-buckets {
        maxSize = 128
        ttl = 1 hour
    }
}

# Specifies thread pools used by the biz platform
async.executor {
    # This work queue is shared by all transfer managers across all object stores and
    # used for multipart up- and downloads.
    s3 {
        poolSize = 10
        queueLength = 0
    }
}

# By default we use the smart lock manager. This detects the presence of redis and uses cluster-wide locks
# or otherwise uses fast local locks within the JVM. to enforce local locks, use "java".
# Another approach for clusters without Redis is using an SQL Database to implement locks distributed locks
# which is available via "sql" (SQLLockManager).
locks.manager = "smart"

# Determines how "Sequences" are stored and computed. By default a "smart" strategy is used which either
# checks if a "sql" database or a "mongo" database is ready and picks the right strategy. If both are
# available the effective startegy can be determined by setting an explicit value here.
sequences.strategy = "smart"

importer.aliases {
    sqluseraccount {
        userAccountData_email: [ "email", "$Model.email" ]
        userAccountData_externalLoginRequired: [ "externalLoginRequired", "$UserAccount.externalLoginRequired" ]
        userAccountData_person_title: [ "lastname", "$PersonData.title" ]
        userAccountData_person_salutation: [ "salutation", "$PersonData.salutation" ]
        userAccountData_person_firstname: [ "firstname", "$PersonData.firstname" ]
        userAccountData_person_lastname: [ "lastname", "$PersonData.lastname" ]
        userAccountData_login_username: [ "username", "user", "$LoginData.username" ]
        userAccountData_login_generatedPassword: [ "password", "$LoginData.generatedPassword", "$LoginData.password" ]
        userAccountData_login_accountLocked: [ "locked", "$LoginData.accountLocked" ]
        userAccountData_permissions_permissionString: [ "permissionString", "roles", "permissions", "$PermissionData.permissionString" ]
    }

    mongouseraccount {
        userAccountData_email: [ "email", "$Model.email" ]
        userAccountData_externalLoginRequired: [ "externalLoginRequired", "$UserAccount.externalLoginRequired" ]
        userAccountData_person_title: [ "lastname", "$PersonData.title" ]
        userAccountData_person_salutation: [ "salutation", "$PersonData.salutation" ]
        userAccountData_person_firstname: [ "firstname", "$PersonData.firstname" ]
        userAccountData_person_lastname: [ "lastname", "$PersonData.lastname" ]
        userAccountData_login_username: [ "username", "user", "$LoginData.username" ]
        userAccountData_login_generatedPassword: [ "password", "$LoginData.generatedPassword", "$LoginData.password" ]
        userAccountData_login_accountLocked: [ "locked", "$LoginData.accountLocked" ]
        userAccountData_permissions_permissionString: [ "permissionString", "roles", "permissions", "$PermissionData.permissionString" ]
    }
}
